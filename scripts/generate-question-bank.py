#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "pykakasi",
#     "pyyaml",
# ]
# ///

"""
é¡Œåº«ç”¢ç”Ÿè…³æœ¬

å¾ Zettelkasten å¡ç‰‡çš„ã€Œæ—¥æ–‡è§£é‡‹ã€å€å¡Šæå–é¡Œç›®ï¼Œ
ç”¢ç”Ÿçµæ§‹åŒ–çš„ JSON é¡Œåº«æª”æ¡ˆä¾›è¼¸å…¥ç·´ç¿’ä½¿ç”¨ã€‚

ç”¨æ³•ï¼š
    uv run scripts/generate-question-bank.py
    uv run scripts/generate-question-bank.py --output static/data/questions.json
    uv run scripts/generate-question-bank.py --verbose
"""

import argparse
import json
import re
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path

import pykakasi
import yaml


# ===== ç¾…é¦¬å­—å°æ‡‰è¡¨ =====
# èˆ‡ src/domain/RomajiMap.js ä¿æŒä¸€è‡´

ROMAJI_MAP: dict[str, list[str]] = {
    # æ¯éŸ³
    'ã‚': ['a'], 'ã„': ['i'], 'ã†': ['u'], 'ãˆ': ['e'], 'ãŠ': ['o'],
    'ã‚¢': ['a'], 'ã‚¤': ['i'], 'ã‚¦': ['u'], 'ã‚¨': ['e'], 'ã‚ª': ['o'],

    # ã‹è¡Œ
    'ã‹': ['ka'], 'ã': ['ki'], 'ã': ['ku'], 'ã‘': ['ke'], 'ã“': ['ko'],
    'ã‚«': ['ka'], 'ã‚­': ['ki'], 'ã‚¯': ['ku'], 'ã‚±': ['ke'], 'ã‚³': ['ko'],

    # ã•è¡Œ
    'ã•': ['sa'], 'ã—': ['si', 'shi'], 'ã™': ['su'], 'ã›': ['se'], 'ã': ['so'],
    'ã‚µ': ['sa'], 'ã‚·': ['si', 'shi'], 'ã‚¹': ['su'], 'ã‚»': ['se'], 'ã‚½': ['so'],

    # ãŸè¡Œ
    'ãŸ': ['ta'], 'ã¡': ['ti', 'chi'], 'ã¤': ['tu', 'tsu'], 'ã¦': ['te'], 'ã¨': ['to'],
    'ã‚¿': ['ta'], 'ãƒ': ['ti', 'chi'], 'ãƒ„': ['tu', 'tsu'], 'ãƒ†': ['te'], 'ãƒˆ': ['to'],

    # ãªè¡Œ
    'ãª': ['na'], 'ã«': ['ni'], 'ã¬': ['nu'], 'ã­': ['ne'], 'ã®': ['no'],
    'ãƒŠ': ['na'], 'ãƒ‹': ['ni'], 'ãƒŒ': ['nu'], 'ãƒ': ['ne'], 'ãƒ': ['no'],

    # ã¯è¡Œ
    'ã¯': ['ha'], 'ã²': ['hi'], 'ãµ': ['hu', 'fu'], 'ã¸': ['he'], 'ã»': ['ho'],
    'ãƒ': ['ha'], 'ãƒ’': ['hi'], 'ãƒ•': ['hu', 'fu'], 'ãƒ˜': ['he'], 'ãƒ›': ['ho'],

    # ã¾è¡Œ
    'ã¾': ['ma'], 'ã¿': ['mi'], 'ã‚€': ['mu'], 'ã‚': ['me'], 'ã‚‚': ['mo'],
    'ãƒ': ['ma'], 'ãƒŸ': ['mi'], 'ãƒ ': ['mu'], 'ãƒ¡': ['me'], 'ãƒ¢': ['mo'],

    # ã‚„è¡Œ
    'ã‚„': ['ya'], 'ã‚†': ['yu'], 'ã‚ˆ': ['yo'],
    'ãƒ¤': ['ya'], 'ãƒ¦': ['yu'], 'ãƒ¨': ['yo'],

    # ã‚‰è¡Œ
    'ã‚‰': ['ra'], 'ã‚Š': ['ri'], 'ã‚‹': ['ru'], 'ã‚Œ': ['re'], 'ã‚': ['ro'],
    'ãƒ©': ['ra'], 'ãƒª': ['ri'], 'ãƒ«': ['ru'], 'ãƒ¬': ['re'], 'ãƒ­': ['ro'],

    # ã‚è¡Œ
    'ã‚': ['wa'], 'ã‚’': ['wo'], 'ã‚“': ['n', 'nn'],
    'ãƒ¯': ['wa'], 'ãƒ²': ['wo'], 'ãƒ³': ['n', 'nn'],

    # æ¿éŸ³
    'ãŒ': ['ga'], 'ã': ['gi'], 'ã': ['gu'], 'ã’': ['ge'], 'ã”': ['go'],
    'ã‚¬': ['ga'], 'ã‚®': ['gi'], 'ã‚°': ['gu'], 'ã‚²': ['ge'], 'ã‚´': ['go'],

    'ã–': ['za'], 'ã˜': ['zi', 'ji'], 'ãš': ['zu'], 'ãœ': ['ze'], 'ã': ['zo'],
    'ã‚¶': ['za'], 'ã‚¸': ['zi', 'ji'], 'ã‚º': ['zu'], 'ã‚¼': ['ze'], 'ã‚¾': ['zo'],

    'ã ': ['da'], 'ã¢': ['di'], 'ã¥': ['du', 'dzu'], 'ã§': ['de'], 'ã©': ['do'],
    'ãƒ€': ['da'], 'ãƒ‚': ['di'], 'ãƒ…': ['du', 'dzu'], 'ãƒ‡': ['de'], 'ãƒ‰': ['do'],

    'ã°': ['ba'], 'ã³': ['bi'], 'ã¶': ['bu'], 'ã¹': ['be'], 'ã¼': ['bo'],
    'ãƒ': ['ba'], 'ãƒ“': ['bi'], 'ãƒ–': ['bu'], 'ãƒ™': ['be'], 'ãƒœ': ['bo'],

    # åŠæ¿éŸ³
    'ã±': ['pa'], 'ã´': ['pi'], 'ã·': ['pu'], 'ãº': ['pe'], 'ã½': ['po'],
    'ãƒ‘': ['pa'], 'ãƒ”': ['pi'], 'ãƒ—': ['pu'], 'ãƒš': ['pe'], 'ãƒ': ['po'],

    # æ‹—éŸ³
    'ãã‚ƒ': ['kya'], 'ãã‚…': ['kyu'], 'ãã‚‡': ['kyo'],
    'ã—ã‚ƒ': ['sya', 'sha'], 'ã—ã‚…': ['syu', 'shu'], 'ã—ã‚‡': ['syo', 'sho'],
    'ã¡ã‚ƒ': ['tya', 'cha'], 'ã¡ã‚…': ['tyu', 'chu'], 'ã¡ã‚‡': ['tyo', 'cho'],
    'ã«ã‚ƒ': ['nya'], 'ã«ã‚…': ['nyu'], 'ã«ã‚‡': ['nyo'],
    'ã²ã‚ƒ': ['hya'], 'ã²ã‚…': ['hyu'], 'ã²ã‚‡': ['hyo'],
    'ã¿ã‚ƒ': ['mya'], 'ã¿ã‚…': ['myu'], 'ã¿ã‚‡': ['myo'],
    'ã‚Šã‚ƒ': ['rya'], 'ã‚Šã‚…': ['ryu'], 'ã‚Šã‚‡': ['ryo'],
    'ãã‚ƒ': ['gya'], 'ãã‚…': ['gyu'], 'ãã‚‡': ['gyo'],
    'ã˜ã‚ƒ': ['zya', 'ja', 'jya'], 'ã˜ã‚…': ['zyu', 'ju', 'jyu'], 'ã˜ã‚‡': ['zyo', 'jo', 'jyo'],
    'ã³ã‚ƒ': ['bya'], 'ã³ã‚…': ['byu'], 'ã³ã‚‡': ['byo'],
    'ã´ã‚ƒ': ['pya'], 'ã´ã‚…': ['pyu'], 'ã´ã‚‡': ['pyo'],

    # ç‰‡å‡åæ‹—éŸ³
    'ã‚­ãƒ£': ['kya'], 'ã‚­ãƒ¥': ['kyu'], 'ã‚­ãƒ§': ['kyo'],
    'ã‚·ãƒ£': ['sya', 'sha'], 'ã‚·ãƒ¥': ['syu', 'shu'], 'ã‚·ãƒ§': ['syo', 'sho'],
    'ãƒãƒ£': ['tya', 'cha'], 'ãƒãƒ¥': ['tyu', 'chu'], 'ãƒãƒ§': ['tyo', 'cho'],
    'ãƒ‹ãƒ£': ['nya'], 'ãƒ‹ãƒ¥': ['nyu'], 'ãƒ‹ãƒ§': ['nyo'],
    'ãƒ’ãƒ£': ['hya'], 'ãƒ’ãƒ¥': ['hyu'], 'ãƒ’ãƒ§': ['hyo'],
    'ãƒŸãƒ£': ['mya'], 'ãƒŸãƒ¥': ['myu'], 'ãƒŸãƒ§': ['myo'],
    'ãƒªãƒ£': ['rya'], 'ãƒªãƒ¥': ['ryu'], 'ãƒªãƒ§': ['ryo'],
    'ã‚®ãƒ£': ['gya'], 'ã‚®ãƒ¥': ['gyu'], 'ã‚®ãƒ§': ['gyo'],
    'ã‚¸ãƒ£': ['zya', 'ja', 'jya'], 'ã‚¸ãƒ¥': ['zyu', 'ju', 'jyu'], 'ã‚¸ãƒ§': ['zyo', 'jo', 'jyo'],
    'ãƒ“ãƒ£': ['bya'], 'ãƒ“ãƒ¥': ['byu'], 'ãƒ“ãƒ§': ['byo'],
    'ãƒ”ãƒ£': ['pya'], 'ãƒ”ãƒ¥': ['pyu'], 'ãƒ”ãƒ§': ['pyo'],

    # ä¿ƒéŸ³
    'ã£': ['xtu', 'ltu', 'xtsu', 'ltsu'],
    'ãƒƒ': ['xtu', 'ltu', 'xtsu', 'ltsu'],

    # å°å­—
    'ã': ['xa', 'la'], 'ãƒ': ['xi', 'li'], 'ã…': ['xu', 'lu'], 'ã‡': ['xe', 'le'], 'ã‰': ['xo', 'lo'],
    'ã‚ƒ': ['xya', 'lya'], 'ã‚…': ['xyu', 'lyu'], 'ã‚‡': ['xyo', 'lyo'],
    'ã‚¡': ['xa', 'la'], 'ã‚£': ['xi', 'li'], 'ã‚¥': ['xu', 'lu'], 'ã‚§': ['xe', 'le'], 'ã‚©': ['xo', 'lo'],
    'ãƒ£': ['xya', 'lya'], 'ãƒ¥': ['xyu', 'lyu'], 'ãƒ§': ['xyo', 'lyo'],

    # æ¨™é»ç¬¦è™Ÿ
    'ã€': [','], 'ã€‚': ['.'], 'ï¼Ÿ': ['?'], 'ï¼': ['!'],
    'ã€Œ': ['['], 'ã€': [']'],
    'ãƒ¼': ['-'],

    # å¤–ä¾†èªç‰‡å‡å
    'ãƒ†ã‚£': ['thi', 'texi', 'teli'],
    'ãƒ‡ã‚£': ['dhi', 'dexi', 'deli'],
    'ãƒ•ã‚¡': ['fa', 'huxa', 'hula'],
    'ãƒ•ã‚£': ['fi', 'huxi', 'huli'],
    'ãƒ•ã‚§': ['fe', 'huxe', 'hule'],
    'ãƒ•ã‚©': ['fo', 'huxo', 'hulo'],
    'ã‚¦ã‚£': ['wi', 'uxi', 'uli'],
    'ã‚¦ã‚§': ['we', 'uxe', 'ule'],
    'ã‚¦ã‚©': ['wo', 'uxo', 'ulo'],
    'ãƒ´ã‚¡': ['va', 'vuxa'],
    'ãƒ´ã‚£': ['vi', 'vuxi'],
    'ãƒ´': ['vu'],
    'ãƒ´ã‚§': ['ve', 'vuxe'],
    'ãƒ´ã‚©': ['vo', 'vuxo'],

    # å¤å…¸å‡å
    'ã‚': ['wi'], 'ãƒ°': ['wi'],
    'ã‚‘': ['we'], 'ãƒ±': ['we'],
}

# ä¿ƒéŸ³å­—å…ƒ
SOKUON = {'ã£', 'ãƒƒ'}

# æ‹—éŸ³å°å­—
YOUON_SMALL = {'ã‚ƒ', 'ã‚…', 'ã‚‡', 'ãƒ£', 'ãƒ¥', 'ãƒ§'}


@dataclass
class CardInfo:
    """å¡ç‰‡è³‡è¨Š"""
    path: Path
    category: str
    number: str
    title: str
    jlpt: str = 'n5'
    japanese_explanation: str = ''


@dataclass
class Character:
    """å­—å…ƒè³‡è¨Š"""
    display: str  # é¡¯ç¤ºæ–‡å­—ï¼ˆå¯èƒ½æ˜¯æ¼¢å­—ï¼‰
    kana: str     # å‡åï¼ˆç”¨æ–¼è½‰æ›ç¾…é¦¬å­—ï¼‰
    romaji: list[str] = field(default_factory=list)


@dataclass
class Question:
    """é¡Œç›®"""
    id: str
    text: str
    characters: list[dict]
    source: dict
    metadata: dict


class QuestionGenerator:
    """é¡Œç›®ç”¢ç”Ÿå™¨"""

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.kks = pykakasi.kakasi()
        self.warnings: list[str] = []

    def log(self, msg: str) -> None:
        """è¼¸å‡ºæ—¥èªŒ"""
        if self.verbose:
            print(msg)

    def warn(self, msg: str) -> None:
        """è¨˜éŒ„è­¦å‘Š"""
        self.warnings.append(msg)
        if self.verbose:
            print(f"âš ï¸  {msg}")

    def scan_cards(self, zettelkasten_dir: Path) -> list[CardInfo]:
        """æƒææ‰€æœ‰å¡ç‰‡"""
        cards = []

        for category_dir in zettelkasten_dir.iterdir():
            # è·³ééç›®éŒ„ã€ç´¢å¼•æª”ã€meta ç›®éŒ„
            if not category_dir.is_dir():
                continue
            if category_dir.name.startswith('_'):
                continue

            category = category_dir.name

            for card_file in category_dir.glob('*.md'):
                # è·³éç´¢å¼•æª”
                if card_file.name.startswith('_'):
                    continue

                card = self.parse_card(card_file, category)
                if card and card.japanese_explanation:
                    cards.append(card)

        self.log(f"æƒæåˆ° {len(cards)} å¼µæœ‰æ—¥æ–‡è§£é‡‹çš„å¡ç‰‡")
        return cards

    def parse_card(self, card_path: Path, category: str) -> CardInfo | None:
        """è§£æå–®å¼µå¡ç‰‡"""
        try:
            content = card_path.read_text(encoding='utf-8')
        except Exception as e:
            self.warn(f"ç„¡æ³•è®€å– {card_path}: {e}")
            return None

        # è§£æ frontmatter
        frontmatter = self.extract_frontmatter(content)
        if not frontmatter:
            return None

        # è·³éè‰ç¨¿å¡ç‰‡ï¼ˆHugo ä¸æœƒç™¼å¸ƒ draft: true çš„å¡ç‰‡ï¼‰
        if frontmatter.get('draft', False):
            self.log(f"è·³éè‰ç¨¿: {card_path.name}")
            return None

        # æå–ç·¨è™Ÿ
        match = re.match(r'^(\d+)_', card_path.stem)
        number = match.group(1) if match else '000'

        # æå–æ—¥æ–‡è§£é‡‹
        jp_explanation = self.extract_japanese_explanation(content)

        return CardInfo(
            path=card_path,
            category=category,
            number=number,
            title=frontmatter.get('title', card_path.stem),
            jlpt=self.normalize_jlpt(frontmatter.get('jlpt', 'n5')),
            japanese_explanation=jp_explanation,
        )

    def extract_frontmatter(self, content: str) -> dict | None:
        """æå– YAML frontmatter"""
        match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
        if not match:
            return None

        try:
            return yaml.safe_load(match.group(1))
        except yaml.YAMLError:
            return None

    def extract_japanese_explanation(self, content: str) -> str:
        """æå–æ—¥æ–‡è§£é‡‹å€å¡Š"""
        # å°‹æ‰¾ ## æ—¥æ–‡è§£é‡‹ æ¨™é¡Œ
        match = re.search(r'^## æ—¥æ–‡è§£é‡‹\s*\n\n(.+?)(?=\n## |\n---|\Z)', content, re.MULTILINE | re.DOTALL)
        if not match:
            return ''

        text = match.group(1).strip()

        # æ¸…ç†æ–‡å­—
        text = self.clean_japanese_text(text)

        # å¦‚æœå¤ªé•·ï¼Œå–ç¬¬ä¸€å¥
        if len(text) > 200:
            # åœ¨å¥è™Ÿè™•æˆªæ–·
            first_sentence_match = re.match(r'^.+?ã€‚', text)
            if first_sentence_match:
                text = first_sentence_match.group(0)

        return text

    def clean_japanese_text(self, text: str) -> str:
        """æ¸…ç†æ—¥æ–‡æ–‡å­—"""
        # ç§»é™¤ markdown è…³è¨»æ¨™è¨˜ [^xxx]
        text = re.sub(r'\[\^[^\]]+\]', '', text)

        # ç§»é™¤æ‹¬è™Ÿå…§çš„è‹±æ–‡è¨»è§£ï¼ˆä½†ä¿ç•™æ—¥æ–‡æ‹¬è™Ÿå…§å®¹ï¼‰
        # ä¾‹å¦‚ï¼šã€Œæœã€ï¼ˆã‚ã•ã€morningï¼‰ â†’ ã€Œæœã€ï¼ˆã‚ã•ï¼‰
        text = re.sub(r'ï¼ˆ[^ï¼‰]*[a-zA-Z][^ï¼‰]*ï¼‰', '', text)
        text = re.sub(r'\([^)]*[a-zA-Z][^)]*\)', '', text)

        # ç§»é™¤é€£çºŒç©ºç™½
        text = re.sub(r'\s+', '', text)

        return text.strip()

    def normalize_jlpt(self, jlpt: str) -> str:
        """æ¨™æº–åŒ– JLPT ç­‰ç´š"""
        if isinstance(jlpt, str):
            jlpt = jlpt.lower().strip()
            if jlpt in ['n5', 'n4', 'n3', 'n2', 'n1']:
                return jlpt
        return 'n5'

    def process_card(self, card: CardInfo) -> Question | None:
        """è™•ç†å–®å¼µå¡ç‰‡ï¼Œç”¢ç”Ÿé¡Œç›®"""
        text = card.japanese_explanation
        if not text:
            return None

        # ç”¢ç”Ÿå­—å…ƒåˆ†è§£
        characters = self.decompose_text(text)
        if not characters:
            self.warn(f"ç„¡æ³•åˆ†è§£æ–‡å­—: {card.path}")
            return None

        # è¨ˆç®— metadata
        has_kanji = any(self.is_kanji(c['display']) for c in characters)
        difficulty = self.calculate_difficulty(text, has_kanji)

        return Question(
            id=f"{card.category}-{card.number}",
            text=text,
            characters=characters,
            source={
                'path': f"/{card.category}/{card.path.stem}/",
                'title': card.title,
                'category': card.category,
                'jlpt': card.jlpt,
            },
            metadata={
                'characterCount': len(characters),
                'difficulty': difficulty,
                'hasKanji': has_kanji,
            },
        )

    def decompose_text(self, text: str) -> list[dict]:
        """åˆ†è§£æ–‡å­—ç‚ºå­—å…ƒé™£åˆ—"""
        # ä½¿ç”¨ pykakasi å–å¾—è®€éŸ³è³‡è¨Šï¼ˆä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
        result = self.kks.convert(text)

        characters = []

        for item in result:
            orig = item['orig']    # åŸå§‹æ–‡å­—
            hira = item['hira']    # å¹³å‡å

            # é€å­—è™•ç†ï¼Œä¸¦ä½¿ç”¨ pykakasi æä¾›çš„è®€éŸ³
            chars = self.process_segment(orig, hira)
            characters.extend(chars)

        return characters

    def process_segment(self, orig: str, hira: str) -> list[dict]:
        """è™•ç†ä¸€å€‹ç‰‡æ®µï¼ˆå¯èƒ½æ˜¯å–®å­—æˆ–è©çµ„ï¼‰"""
        chars = []

        # å¦‚æœåŸæ–‡èˆ‡å‡åç›¸åŒï¼ˆç´”å‡åæˆ–ç¬¦è™Ÿï¼‰ï¼Œç›´æ¥é€å­—è™•ç†
        if orig == hira:
            i = 0
            while i < len(orig):
                char, consumed = self.get_kana_with_romaji(orig, i)
                chars.append(char)
                i += consumed
            return chars

        # æ¼¢å­—æˆ–æ··åˆæ–‡å­—ï¼šç²¾ç¢ºå°é½ŠåŸæ–‡èˆ‡å‡åè®€éŸ³
        chars = self.align_kanji_kana_precise(orig, hira)
        return chars

    def align_kanji_kana_precise(self, orig: str, hira: str) -> list[dict]:
        """ç²¾ç¢ºå°é½Šæ¼¢å­—èˆ‡å‡åè®€éŸ³

        ç­–ç•¥ï¼šæ‰¾å‡ºåŸæ–‡ä¸­çš„å‡åã€ŒéŒ¨é»ã€ï¼Œç”¨å®ƒå€‘ä¾†åˆ†å‰²å‡åè®€éŸ³
        ä¾‹å¦‚ï¼šã€Œè¦šãˆã‚‹ã€(orig) èˆ‡ã€ŒãŠã¼ãˆã‚‹ã€(hira)
        - ã€Œãˆã‚‹ã€æ˜¯éŒ¨é»ï¼Œå¯ä»¥ç¢ºå®šã€Œè¦šã€= ã€ŒãŠã¼ã€
        """
        chars = []

        # åˆ†æåŸæ–‡çµæ§‹ï¼Œæ‰¾å‡ºæ¼¢å­—å€å¡Šå’Œå‡åéŒ¨é»
        segments = self.segment_text(orig, hira)

        for segment in segments:
            if segment['type'] == 'kana':
                # å‡åç›´æ¥é€å­—è™•ç†
                text = segment['text']
                i = 0
                while i < len(text):
                    char_data, consumed = self.get_kana_with_romaji(text, i)
                    chars.append(char_data)
                    i += consumed

            elif segment['type'] == 'kanji':
                # æ¼¢å­—å€å¡Šï¼šä½¿ç”¨å®Œæ•´è®€éŸ³
                kanji_text = segment['text']
                kana_reading = segment['kana']
                romaji = self.build_romaji_for_kana(kana_reading)

                # å¦‚æœåªæœ‰ä¸€å€‹æ¼¢å­—ï¼Œç›´æ¥ä½¿ç”¨
                if len(kanji_text) == 1:
                    chars.append({
                        'display': kanji_text,
                        'kana': kana_reading,
                        'romaji': romaji,
                    })
                else:
                    # å¤šå€‹æ¼¢å­—ï¼šä½œç‚ºä¸€å€‹æ•´é«”è¼¸å…¥å–®ä½
                    chars.append({
                        'display': kanji_text,
                        'kana': kana_reading,
                        'romaji': romaji,
                    })

            elif segment['type'] == 'other':
                # å…¶ä»–å­—å…ƒ
                char = segment['text']
                if char in ROMAJI_MAP:
                    chars.append({
                        'display': char,
                        'kana': char,
                        'romaji': ROMAJI_MAP[char],
                    })
                else:
                    chars.append({
                        'display': char,
                        'kana': char,
                        'romaji': [char],
                    })

        return chars

    def segment_text(self, orig: str, hira: str) -> list[dict]:
        """å°‡åŸæ–‡åˆ†å‰²ç‚ºå‡åã€æ¼¢å­—å€å¡Šã€å…¶ä»–å­—å…ƒ

        è¿”å›ï¼š[{'type': 'kana'|'kanji'|'other', 'text': str, 'kana': str}, ...]
        """
        segments = []
        hira_pos = 0

        i = 0
        while i < len(orig):
            char = orig[i]

            if self.is_kana(char) or char in ROMAJI_MAP:
                # å‡åæˆ–æ¨™é»ï¼šæ”¶é›†é€£çºŒçš„å‡å
                kana_start = i
                while i < len(orig) and (self.is_kana(orig[i]) or orig[i] in ROMAJI_MAP):
                    i += 1
                kana_text = orig[kana_start:i]

                # å°æ‡‰çš„å‡åè®€éŸ³æ‡‰è©²ç›¸åŒ
                hira_len = len(kana_text)
                hira_part = hira[hira_pos:hira_pos + hira_len]
                hira_pos += hira_len

                segments.append({
                    'type': 'kana',
                    'text': kana_text,
                    'kana': hira_part,
                })

            elif self.is_kanji(char):
                # æ¼¢å­—ï¼šæ”¶é›†é€£çºŒçš„æ¼¢å­—
                kanji_start = i
                while i < len(orig) and self.is_kanji(orig[i]):
                    i += 1
                kanji_text = orig[kanji_start:i]

                # æ‰¾å‡ºå°æ‡‰çš„å‡åè®€éŸ³
                # æ–¹æ³•ï¼šæ‰¾åˆ°ä¸‹ä¸€å€‹å‡åéŒ¨é»
                if i < len(orig):
                    # æ‰¾ä¸‹ä¸€å€‹åŸæ–‡ä¸­çš„å‡å
                    next_kana_in_orig = None
                    for c in orig[i:]:
                        if self.is_kana(c):
                            next_kana_in_orig = c
                            break

                    if next_kana_in_orig and next_kana_in_orig in hira[hira_pos:]:
                        # æ‰¾åˆ°éŒ¨é»ï¼Œç¢ºå®šæ¼¢å­—è®€éŸ³çš„ç¯„åœ
                        anchor_in_hira = hira.find(next_kana_in_orig, hira_pos)
                        kana_reading = hira[hira_pos:anchor_in_hira]
                        hira_pos = anchor_in_hira
                    else:
                        # æ²’æœ‰éŒ¨é»ï¼Œä½¿ç”¨å‰©é¤˜å…¨éƒ¨
                        kana_reading = hira[hira_pos:]
                        hira_pos = len(hira)
                else:
                    # æ¼¢å­—åœ¨æœ«å°¾
                    kana_reading = hira[hira_pos:]
                    hira_pos = len(hira)

                segments.append({
                    'type': 'kanji',
                    'text': kanji_text,
                    'kana': kana_reading,
                })

            else:
                # å…¶ä»–å­—å…ƒï¼ˆæ•¸å­—ã€è‹±æ–‡ç­‰ï¼‰
                segments.append({
                    'type': 'other',
                    'text': char,
                    'kana': char,
                })
                i += 1
                # å‡åä½ç½®ä¸è®Šï¼ˆå…¶ä»–å­—å…ƒæ²’æœ‰å°æ‡‰è®€éŸ³ï¼‰

        return segments

    def build_romaji_for_kana(self, kana: str) -> list[str]:
        """ç‚ºå‡åå­—ä¸²å»ºç«‹ç¾…é¦¬å­—é¸é …

        å›å‚³æ‰€æœ‰å¯èƒ½çš„ç¾…é¦¬å­—çµ„åˆï¼ˆæœ€å¤šä¿ç•™å‰ 4 å€‹ï¼‰
        """
        if not kana:
            return []

        # å–®å­—å…ƒç›´æ¥æŸ¥è¡¨
        if len(kana) == 1 and kana in ROMAJI_MAP:
            return ROMAJI_MAP[kana]

        # å¤šå­—å…ƒï¼šä½¿ç”¨ DFS çµ„åˆå„å­—å…ƒçš„æ‰€æœ‰ç¾…é¦¬å­—é¸é …
        all_options = self.generate_romaji_combinations(kana)

        # é™åˆ¶è¿”å›æ•¸é‡ï¼Œé¿å…çµ„åˆçˆ†ç‚¸
        return all_options[:4] if all_options else [kana]

    def generate_romaji_combinations(self, kana: str) -> list[str]:
        """ä½¿ç”¨ DFS ç”Ÿæˆæ‰€æœ‰ç¾…é¦¬å­—çµ„åˆ"""
        if not kana:
            return ['']

        results = []

        # å˜—è©¦å…©å­—çµ„åˆï¼ˆæ‹—éŸ³ï¼‰
        if len(kana) >= 2 and kana[:2] in ROMAJI_MAP:
            for option in ROMAJI_MAP[kana[:2]]:
                for suffix in self.generate_romaji_combinations(kana[2:]):
                    results.append(option + suffix)

        # å˜—è©¦å–®å­—
        if kana[0] in ROMAJI_MAP:
            for option in ROMAJI_MAP[kana[0]]:
                for suffix in self.generate_romaji_combinations(kana[1:]):
                    results.append(option + suffix)
        elif not results:
            # ç„¡æ³•åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨åŸå­—å…ƒ
            for suffix in self.generate_romaji_combinations(kana[1:]):
                results.append(kana[0] + suffix)

        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        seen = set()
        unique = []
        for r in results:
            if r not in seen:
                seen.add(r)
                unique.append(r)
            if len(unique) >= 8:  # å…§éƒ¨é™åˆ¶
                break

        return unique

    def get_kana_with_romaji(self, text: str, pos: int) -> tuple[dict, int]:
        """å–å¾—å‡ååŠå…¶ç¾…é¦¬å­—"""
        # å„ªå…ˆæª¢æŸ¥å…©å­—çµ„åˆï¼ˆæ‹—éŸ³ç­‰ï¼‰
        if pos + 1 < len(text):
            two_char = text[pos:pos + 2]
            if two_char in ROMAJI_MAP:
                return {
                    'display': two_char,
                    'kana': two_char,
                    'romaji': ROMAJI_MAP[two_char],
                }, 2

        # å–®å­—è™•ç†
        char = text[pos]

        # æª¢æŸ¥æ˜¯å¦æœ‰ç¾…é¦¬å­—å°æ‡‰
        if char in ROMAJI_MAP:
            return {
                'display': char,
                'kana': char,
                'romaji': ROMAJI_MAP[char],
            }, 1

        # å…¶ä»–å­—å…ƒï¼ˆæ¼¢å­—ã€è‹±æ•¸å­—ç­‰ï¼‰ç›´æ¥ä¿ç•™
        return {
            'display': char,
            'kana': char,
            'romaji': [char],  # ç›´æ¥è¼¸å…¥åŸå­—å…ƒ
        }, 1

    def is_kanji(self, text: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦åŒ…å«æ¼¢å­—"""
        for char in text:
            code = ord(char)
            if (0x4E00 <= code <= 0x9FFF or  # CJK çµ±ä¸€æ¼¢å­—
                0x3400 <= code <= 0x4DBF or  # CJK çµ±ä¸€æ¼¢å­—æ“´å±• A
                0xF900 <= code <= 0xFAFF):   # CJK ç›¸å®¹æ¼¢å­—
                return True
        return False

    def is_kana(self, char: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºå‡å"""
        code = ord(char)
        return (0x3040 <= code <= 0x309F or  # å¹³å‡å
                0x30A0 <= code <= 0x30FF)    # ç‰‡å‡å

    def calculate_difficulty(self, text: str, has_kanji: bool) -> str:
        """è¨ˆç®—é›£åº¦"""
        length = len(text)

        if length < 30 and not has_kanji:
            return 'easy'
        elif length < 60:
            return 'medium'
        else:
            return 'hard'

    def calculate_stats(self, questions: list[Question]) -> dict:
        """è¨ˆç®—çµ±è¨ˆè³‡è¨Š"""
        by_jlpt: dict[str, int] = {}
        by_category: dict[str, int] = {}
        by_difficulty: dict[str, int] = {}

        for q in questions:
            jlpt = q.source['jlpt']
            category = q.source['category']
            difficulty = q.metadata['difficulty']

            by_jlpt[jlpt] = by_jlpt.get(jlpt, 0) + 1
            by_category[category] = by_category.get(category, 0) + 1
            by_difficulty[difficulty] = by_difficulty.get(difficulty, 0) + 1

        return {
            'totalQuestions': len(questions),
            'byJlpt': dict(sorted(by_jlpt.items())),
            'byCategory': dict(sorted(by_category.items())),
            'byDifficulty': by_difficulty,
        }

    def generate(self, zettelkasten_dir: Path, output_path: Path) -> int:
        """ç”¢ç”Ÿé¡Œåº«"""
        # æƒæå¡ç‰‡
        cards = self.scan_cards(zettelkasten_dir)

        # ç”¢ç”Ÿé¡Œç›®
        questions = []
        for card in cards:
            question = self.process_card(card)
            if question:
                questions.append(question)
                self.log(f"âœ“ {question.id}: {question.text[:30]}...")

        # çµ„è£è¼¸å‡º
        output = {
            'version': '1.1.0',
            'generated': datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
            'questions': [
                {
                    'id': q.id,
                    'text': q.text,
                    'characters': q.characters,
                    'source': q.source,
                    'metadata': q.metadata,
                }
                for q in questions
            ],
            'stats': self.calculate_stats(questions),
        }

        # è¼¸å‡º
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, separators=(',', ':'))

        # è¼¸å‡ºçµ±è¨ˆ
        print(f"âœ… ç”¢ç”Ÿ {len(questions)} é¡Œåˆ° {output_path}")
        print(f"   æŒ‰ JLPT: {output['stats']['byJlpt']}")
        print(f"   æŒ‰åˆ†é¡: {len(output['stats']['byCategory'])} é¡")

        if self.warnings:
            print(f"\nâš ï¸  {len(self.warnings)} å€‹è­¦å‘Š")

        return len(questions)

    def generate_split(self, zettelkasten_dir: Path, output_dir: Path, init_count: int = 30) -> int:
        """ç”¢ç”Ÿåˆ†å‰²é¡Œåº«ï¼ˆæ¼¸é€²å¼è¼‰å…¥ç”¨ï¼‰

        Args:
            zettelkasten_dir: Zettelkasten ç›®éŒ„è·¯å¾‘
            output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
            init_count: åˆå§‹åŒ…çš„é¡Œç›®æ•¸é‡ï¼ˆé è¨­ 30ï¼‰

        Returns:
            ç¸½é¡Œç›®æ•¸é‡
        """
        # æƒæå¡ç‰‡
        cards = self.scan_cards(zettelkasten_dir)

        # ç”¢ç”Ÿé¡Œç›®
        questions = []
        for card in cards:
            question = self.process_card(card)
            if question:
                questions.append(question)
                self.log(f"âœ“ {question.id}: {question.text[:30]}...")

        # æŒ‰ JLPT åˆ†çµ„
        by_jlpt: dict[str, list[Question]] = {}
        for q in questions:
            jlpt = q.source['jlpt']
            if jlpt not in by_jlpt:
                by_jlpt[jlpt] = []
            by_jlpt[jlpt].append(q)

        # æ’åºæ¯å€‹ç­‰ç´šçš„é¡Œç›®ï¼ˆæŒ‰é›£åº¦ï¼šeasy < medium < hardï¼‰
        difficulty_order = {'easy': 0, 'medium': 1, 'hard': 2}
        for jlpt in by_jlpt:
            by_jlpt[jlpt].sort(key=lambda q: difficulty_order.get(q.metadata['difficulty'], 1))

        # å»ºç«‹åˆå§‹åŒ…ï¼ˆå¾ N5 å–æœ€ç°¡å–®çš„é¡Œç›®ï¼‰
        init_questions = []
        if 'n5' in by_jlpt:
            init_questions = by_jlpt['n5'][:init_count]

        # ç”¢ç”Ÿæ™‚é–“æˆ³
        generated = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')

        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_dir.mkdir(parents=True, exist_ok=True)

        # 1. è¼¸å‡ºç´¢å¼•æª”
        index = {
            'version': '1.2.0',
            'generated': generated,
            'bundles': {
                'init': {
                    'path': 'questions-init.json',
                    'count': len(init_questions),
                    'jlpt': ['n5'],
                    'description': 'Initial bundle for fast startup',
                },
            },
            'stats': self.calculate_stats(questions),
        }

        # ç‚ºæ¯å€‹ JLPT ç­‰ç´šæ·»åŠ  bundle è³‡è¨Š
        for jlpt in sorted(by_jlpt.keys()):
            index['bundles'][jlpt] = {
                'path': f'questions-{jlpt}.json',
                'count': len(by_jlpt[jlpt]),
            }

        with open(output_dir / 'questions-index.json', 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç´¢å¼•æª”: questions-index.json")

        # 2. è¼¸å‡ºåˆå§‹åŒ…
        init_output = {
            'version': '1.2.0',
            'generated': generated,
            'bundle': 'init',
            'questions': [self._question_to_dict(q) for q in init_questions],
        }
        with open(output_dir / 'questions-init.json', 'w', encoding='utf-8') as f:
            json.dump(init_output, f, ensure_ascii=False, separators=(',', ':'))
        init_size = (output_dir / 'questions-init.json').stat().st_size / 1024
        print(f"âœ… åˆå§‹åŒ…: questions-init.json ({len(init_questions)} é¡Œ, {init_size:.1f} KB)")

        # 3. è¼¸å‡ºå„ JLPT ç­‰ç´šçš„é¡Œåº«
        for jlpt in sorted(by_jlpt.keys()):
            jlpt_questions = by_jlpt[jlpt]
            jlpt_output = {
                'version': '1.2.0',
                'generated': generated,
                'bundle': jlpt,
                'questions': [self._question_to_dict(q) for q in jlpt_questions],
            }
            output_file = output_dir / f'questions-{jlpt}.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(jlpt_output, f, ensure_ascii=False, separators=(',', ':'))
            file_size = output_file.stat().st_size / 1024
            print(f"âœ… {jlpt.upper()} é¡Œåº«: questions-{jlpt}.json ({len(jlpt_questions)} é¡Œ, {file_size:.1f} KB)")

        # 4. åŒæ™‚è¼¸å‡ºå®Œæ•´é¡Œåº«ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
        full_output = {
            'version': '1.1.0',
            'generated': generated,
            'questions': [self._question_to_dict(q) for q in questions],
            'stats': self.calculate_stats(questions),
        }
        with open(output_dir / 'questions.json', 'w', encoding='utf-8') as f:
            json.dump(full_output, f, ensure_ascii=False, separators=(',', ':'))
        full_size = (output_dir / 'questions.json').stat().st_size / 1024
        print(f"âœ… å®Œæ•´é¡Œåº«: questions.json ({len(questions)} é¡Œ, {full_size:.1f} KB)")

        # è¼¸å‡ºçµ±è¨ˆ
        print(f"\nğŸ“Š çµ±è¨ˆ:")
        print(f"   ç¸½é¡Œæ•¸: {len(questions)}")
        print(f"   æŒ‰ JLPT: {dict(sorted({k: len(v) for k, v in by_jlpt.items()}.items()))}")
        print(f"   åˆå§‹åŒ…: {len(init_questions)} é¡Œ ({init_size:.1f} KB)")

        if self.warnings:
            print(f"\nâš ï¸  {len(self.warnings)} å€‹è­¦å‘Š")

        return len(questions)

    def _question_to_dict(self, q: Question) -> dict:
        """å°‡ Question è½‰æ›ç‚ºå­—å…¸"""
        return {
            'id': q.id,
            'text': q.text,
            'characters': q.characters,
            'source': q.source,
            'metadata': q.metadata,
        }


def main():
    parser = argparse.ArgumentParser(description='ç”¢ç”Ÿæ—¥æ–‡è¼¸å…¥ç·´ç¿’é¡Œåº«')
    parser.add_argument(
        '--output', '-o',
        default='static/data/questions.json',
        help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆé è¨­ï¼šstatic/data/questions.jsonï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        default='static/data',
        help='åˆ†å‰²è¼¸å‡ºç›®éŒ„ï¼ˆé…åˆ --split ä½¿ç”¨ï¼Œé è¨­ï¼šstatic/dataï¼‰'
    )
    parser.add_argument(
        '--zettelkasten', '-z',
        default='zettelkasten',
        help='Zettelkasten ç›®éŒ„è·¯å¾‘ï¼ˆé è¨­ï¼šzettelkastenï¼‰'
    )
    parser.add_argument(
        '--split',
        action='store_true',
        help='ç”¢ç”Ÿåˆ†å‰²é¡Œåº«ï¼ˆæ¼¸é€²å¼è¼‰å…¥ç”¨ï¼‰'
    )
    parser.add_argument(
        '--init-count',
        type=int,
        default=30,
        help='åˆå§‹åŒ…çš„é¡Œç›®æ•¸é‡ï¼ˆé è¨­ï¼š30ï¼‰'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='é¡¯ç¤ºè©³ç´°è¼¸å‡º'
    )
    args = parser.parse_args()

    generator = QuestionGenerator(verbose=args.verbose)

    if args.split:
        # åˆ†å‰²æ¨¡å¼ï¼šç”¢ç”Ÿå¤šå€‹æª”æ¡ˆ
        generator.generate_split(
            zettelkasten_dir=Path(args.zettelkasten),
            output_dir=Path(args.output_dir),
            init_count=args.init_count,
        )
    else:
        # å‚³çµ±æ¨¡å¼ï¼šç”¢ç”Ÿå–®ä¸€æª”æ¡ˆ
        generator.generate(
            zettelkasten_dir=Path(args.zettelkasten),
            output_path=Path(args.output),
        )


if __name__ == '__main__':
    main()
