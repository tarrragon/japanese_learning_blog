#!/usr/bin/env python3
# /// script
# dependencies = []
# requires-python = ">=3.10"
# ///

"""
replace_pending_links.py - è‡ªå‹•æ›¿æ›ã€Œå¾…å»ºç«‹ã€é€£çµæ¨™è¨˜

ç”¨é€”ï¼šæƒææ‰€æœ‰å¡ç‰‡ï¼Œå°‡ã€Œå¾…å»ºç«‹ã€æ¨™è¨˜æ›¿æ›ç‚ºå¯¦éš›é€£çµè·¯å¾‘
åŸ·è¡Œï¼šuv run scripts/replace_pending_links.py [é¸é …]

é¸é …ï¼š
  --check             æª¢æŸ¥æ¨¡å¼ï¼šçµ±è¨ˆå¾…å»ºç«‹é€£çµæ•¸é‡ï¼ˆé è¨­ï¼‰
  --dry-run           é è¦½è®Šæ›´ï¼Œä¸å¯¦éš›ä¿®æ”¹æª”æ¡ˆ
  --fix               åŸ·è¡Œæ›¿æ›
  --report            ç”Ÿæˆç¼ºå£å ±å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰
  --json              è¼¸å‡º JSON æ ¼å¼çš„ç¼ºå£æ¸…å–®
  --add-to-csv        ç›´æ¥æ–°å¢ç¼ºå£åˆ° Active CSV
  --category CAT      åªè™•ç†ç‰¹å®šåˆ†é¡
  --quiet             éœé»˜æ¨¡å¼

å·¥ä½œæµç¨‹æ•´åˆï¼ˆéšæ®µ 3ï¼šLink Buildingï¼‰ï¼š
  uv run scripts/replace_pending_links.py --fix --add-to-csv --report

æ•ˆç‡å„ªåŒ–ï¼š
  - é å»ºå¡ç‰‡ç´¢å¼•ï¼ˆjapanese â†’ pathï¼‰ï¼ŒO(1) æŸ¥æ‰¾
  - åªè®€å‰ 30 è¡Œå»ºç«‹ç´¢å¼•
  - è¡Œç´šæ›¿æ›ï¼Œä¸è§£ææ•´å€‹ Markdown
"""

import argparse
import json
import re
import sys
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Optional

# åˆ†é¡é—œéµå­—æ˜ å°„ï¼ˆç”¨æ–¼çŒœæ¸¬ç¼ºå£å¡ç‰‡çš„åˆ†é¡ï¼‰
CATEGORY_KEYWORDS = {
    "grammar": ["å½¢", "å‹•è©", "åŠ©è©", "è¡¨ç¾", "æ–‡æ³•", "æ¥ç¶š", "å½¢å¼", "ç”¨æ³•"],
    "concept": ["æ¦‚å¿µ", "ç†è«–", "ç³»çµ±", "ç‰¹å¾´", "æ§‹é€ ", "åŸå‰‡"],
    "noun": ["åè©", "èª", "è©"],
    "verb-ru": ["ã‚‹å‹•è©", "ä¸€æ®µ"],
    "verb-u": ["ã†å‹•è©", "äº”æ®µ"],
    "keigo": ["æ•¬èª", "å°Šæ•¬", "è¬™è­²", "ä¸å¯§"],
    "particle": ["åŠ©è©"],
}

# å°ˆæ¡ˆæ ¹ç›®éŒ„
PROJECT_ROOT = Path(__file__).parent.parent
ZETTELKASTEN_DIR = PROJECT_ROOT / "zettelkasten"

# ä¸æƒæçš„ç›®éŒ„
EXCLUDE_DIRS = {"_meta", ".DS_Store"}

# å¾…å»ºç«‹æ¨™è¨˜çš„æ­£å‰‡è¡¨é”å¼ï¼ˆå¾ detect_pending_links.py è¤‡ç”¨ï¼‰
PENDING_PATTERNS = [
    # [æ–‡å­—](å¾…å»ºç«‹) æˆ– [æ–‡å­—](ã€å¾…å»ºç«‹ã€‘)
    re.compile(r'\[([^\]]+)\]\((å¾…å»ºç«‹|ã€å¾…å»ºç«‹ã€‘)\)'),
    # [å¾…å»ºç«‹](pending) æˆ– [å¾…å»ºç«‹: xxx](...)
    re.compile(r'\[å¾…å»ºç«‹[ï¼š:\s]*([^\]]*)\]\([^)]*\)'),
    # [æ–‡å­—](path)ï¼ˆå¾…å»ºç«‹ï¼‰ - é€£çµå¾Œè·Ÿå¾…å»ºç«‹
    re.compile(r'(\[[^\]]+\]\([^)]+\))ï¼ˆå¾…å»ºç«‹ï¼‰'),
]


@dataclass
class CardEntry:
    """å¡ç‰‡ç´¢å¼•é …ç›®"""
    path: Path
    category: str
    number: str
    title: str
    japanese: str
    aliases: list = field(default_factory=list)


@dataclass
class Gap:
    """ç¼ºå£è¨˜éŒ„"""
    text: str
    source_files: list = field(default_factory=list)
    count: int = 0


class CardIndex:
    """å¡ç‰‡ç´¢å¼•ï¼šå»ºç«‹ japanese â†’ path çš„æ˜ å°„"""

    def __init__(self, zettelkasten_dir: Path):
        self.zettelkasten_dir = zettelkasten_dir
        # ä¸»ç´¢å¼•ï¼šæ—¥æ–‡è©å½™ -> CardEntry
        self.japanese_to_card: dict[str, CardEntry] = {}
        # åˆ¥åç´¢å¼•
        self.alias_to_card: dict[str, CardEntry] = {}
        # æ¨™é¡Œç´¢å¼•
        self.title_to_card: dict[str, CardEntry] = {}
        # çµ±è¨ˆ
        self.total_cards = 0

    def build_index(self) -> None:
        """æƒææ‰€æœ‰å¡ç‰‡ï¼Œåªè®€å‰ 30 è¡Œå»ºç«‹ç´¢å¼•"""
        for category_dir in self.zettelkasten_dir.iterdir():
            if not category_dir.is_dir() or category_dir.name in EXCLUDE_DIRS:
                continue

            for card_file in category_dir.glob("*.md"):
                if card_file.name in ("index.md", "_index.md"):
                    continue

                entry = self._parse_card_header(card_file)
                if entry:
                    self._add_to_index(entry)
                    self.total_cards += 1

    def _parse_card_header(self, path: Path) -> Optional[CardEntry]:
        """åªè®€å‰ 30 è¡Œï¼Œè§£æ YAML frontmatter"""
        try:
            lines = []
            with path.open("r", encoding="utf-8") as f:
                for i, line in enumerate(f):
                    if i >= 30:
                        break
                    lines.append(line)

            # è§£æ YAML
            yaml_data = self._extract_yaml(lines)
            title = yaml_data.get("title", "")

            # å¾æ¨™é¡Œæå–æ—¥æ–‡å’Œåˆ¥å
            japanese, aliases = self._extract_japanese_from_title(title)

            # æå–ç·¨è™Ÿ
            number = self._extract_number(path.name)

            return CardEntry(
                path=path,
                category=path.parent.name,
                number=number,
                title=title,
                japanese=japanese,
                aliases=aliases
            )
        except Exception:
            return None

    def _extract_yaml(self, lines: list[str]) -> dict:
        """å¾è¡Œåˆ—è¡¨è§£æ YAML"""
        in_yaml = False
        yaml_lines = []

        for line in lines:
            stripped = line.strip()
            if stripped == "---":
                if not in_yaml:
                    in_yaml = True
                    continue
                else:
                    break
            if in_yaml:
                yaml_lines.append(line)

        data = {}
        for line in yaml_lines:
            if ":" in line and not line.strip().startswith("-"):
                parts = line.split(":", 1)
                key = parts[0].strip()
                value = parts[1].strip().strip('"').strip("'") if len(parts) > 1 else ""
                data[key] = value

        return data

    def _extract_japanese_from_title(self, title: str) -> tuple[str, list[str]]:
        """
        å¾æ¨™é¡Œæå–æ—¥æ–‡è©å½™å’Œåˆ¥å

        ç¯„ä¾‹ï¼š
          "é£Ÿã¹ã‚‹ï¼ˆãŸã¹ã‚‹ï¼‰" -> ("é£Ÿã¹ã‚‹", ["ãŸã¹ã‚‹"])
          "æœã”ã¯ã‚“ï¼ˆã‚ã•ã”ã¯ã‚“ï¼‰" -> ("æœã”ã¯ã‚“", ["ã‚ã•ã”ã¯ã‚“"])
        """
        aliases = []

        # æå–æ‹¬è™Ÿå…§å®¹ä½œç‚ºåˆ¥å
        paren_match = re.search(r'[ï¼ˆ(]([^ï¼‰)]+)[ï¼‰)]', title)
        if paren_match:
            aliases.append(paren_match.group(1))
            main_text = re.sub(r'[ï¼ˆ(][^ï¼‰)]+[ï¼‰)]', '', title).strip()
        else:
            main_text = title.strip()

        # è™•ç†ã€Œãƒ»ã€åˆ†éš”çš„å¤šå€‹è©
        if 'ãƒ»' in main_text:
            parts = main_text.split('ãƒ»')
            main_text = parts[0].strip()
            aliases.extend([p.strip() for p in parts[1:]])

        return main_text, aliases

    def _extract_number(self, filename: str) -> str:
        """å¾æª”åæå–ç·¨è™Ÿ"""
        match = re.match(r'(\d+)_', filename)
        return match.group(1) if match else ""

    def _add_to_index(self, entry: CardEntry) -> None:
        """å°‡å¡ç‰‡åŠ å…¥ç´¢å¼•"""
        # æ­£è¦åŒ–ä¸¦åŠ å…¥ä¸»ç´¢å¼•
        if entry.japanese:
            norm_jp = self._normalize(entry.japanese)
            self.japanese_to_card[norm_jp] = entry

        # åŠ å…¥åˆ¥åç´¢å¼•
        for alias in entry.aliases:
            norm_alias = self._normalize(alias)
            if norm_alias not in self.alias_to_card:
                self.alias_to_card[norm_alias] = entry

        # åŠ å…¥æ¨™é¡Œç´¢å¼•
        if entry.title:
            norm_title = self._normalize(entry.title)
            self.title_to_card[norm_title] = entry

    def _normalize(self, text: str) -> str:
        """æ­£è¦åŒ–æ–‡å­—"""
        return text.strip().replace('ï¼ˆ', '(').replace('ï¼‰', ')')

    def find_card(self, text: str) -> Optional[CardEntry]:
        """æ ¹æ“šæ—¥æ–‡æ–‡å­—æŸ¥æ‰¾å¡ç‰‡"""
        clean_text = self._normalize(text)

        # 1. ä¸»ç´¢å¼•æŸ¥æ‰¾
        if clean_text in self.japanese_to_card:
            return self.japanese_to_card[clean_text]

        # 2. åˆ¥åæŸ¥æ‰¾
        if clean_text in self.alias_to_card:
            return self.alias_to_card[clean_text]

        # 3. æ¨™é¡ŒæŸ¥æ‰¾
        if clean_text in self.title_to_card:
            return self.title_to_card[clean_text]

        # 4. å»é™¤æ‹¬è™Ÿå¾Œå†æŸ¥æ‰¾
        no_paren = re.sub(r'[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]', '', clean_text).strip()
        if no_paren and no_paren != clean_text:
            if no_paren in self.japanese_to_card:
                return self.japanese_to_card[no_paren]

        return None


class GapReport:
    """ç¼ºå£å ±å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        self.gaps: dict[str, Gap] = {}

    def add(self, text: str, source_file: str) -> None:
        """è¨˜éŒ„ä¸€å€‹ç¼ºå£"""
        if text not in self.gaps:
            self.gaps[text] = Gap(text=text, source_files=[], count=0)
        self.gaps[text].source_files.append(source_file)
        self.gaps[text].count += 1

    def _calc_priority(self, count: int) -> str:
        """æ ¹æ“šå‡ºç¾æ¬¡æ•¸è¨ˆç®—å„ªå…ˆç´š"""
        if count >= 5:
            return "Critical"
        if count >= 3:
            return "High"
        if count >= 2:
            return "Medium"
        return "Low"

    def _guess_category(self, text: str) -> str:
        """æ ¹æ“šæ–‡å­—å…§å®¹çŒœæ¸¬åˆ†é¡"""
        for category, keywords in CATEGORY_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text:
                    return category
        # é è¨­åˆ†é¡
        return "concept"

    def to_json(self) -> list[dict]:
        """ç”Ÿæˆ JSON æ ¼å¼çš„ç¼ºå£æ¸…å–®ï¼ˆä¾› add_pending_cards.py ä½¿ç”¨ï¼‰"""
        date_str = datetime.now().strftime("%Y%m%d")
        sorted_gaps = sorted(
            self.gaps.values(),
            key=lambda g: g.count,
            reverse=True
        )

        cards = []
        for gap in sorted_gaps:
            # éæ¿¾ç„¡æ•ˆçš„ç¼ºå£ï¼ˆå¦‚ç©ºå­—ä¸²ã€ç´”æ¨™è¨˜ç­‰ï¼‰
            if not gap.text or gap.text in ("å¾…å»ºç«‹", "ã€å¾…å»ºç«‹ã€‘", "pending"):
                continue

            cards.append({
                "japanese": gap.text,
                "chinese": "ï¼ˆå¾…å¡«å¯«ï¼‰",  # ä½”ä½ç¬¦ï¼Œå¾…å¡«å¯«
                "category": self._guess_category(gap.text),
                "jlpt": "n3",  # é è¨­å€¼
                "priority": self._calc_priority(gap.count),
                "source": f"gap-{date_str}",
                "stage": "pending",
                "note": f"å‡ºç¾ {gap.count} æ¬¡"
            })

        return cards

    def generate_markdown(self) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼å ±å‘Š"""
        lines = [
            "# å¾…å»ºç«‹å¡ç‰‡ç¼ºå£å ±å‘Š",
            "",
            f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## æ‘˜è¦",
            "",
            f"- ç¼ºå£ç¸½æ•¸ï¼š{len(self.gaps)}",
            f"- ç¸½å‡ºç¾æ¬¡æ•¸ï¼š{sum(g.count for g in self.gaps.values())}",
            "",
            "## æŒ‰å‡ºç¾æ¬¡æ•¸æ’åº",
            "",
            "| æ—¥æ–‡è©å½™ | å‡ºç¾æ¬¡æ•¸ | å„ªå…ˆç´š | å‡ºç¾ä½ç½®ï¼ˆå‰3å€‹ï¼‰|",
            "|----------|----------|--------|------------------|",
        ]

        sorted_gaps = sorted(
            self.gaps.values(),
            key=lambda g: g.count,
            reverse=True
        )

        for gap in sorted_gaps:
            sources = ", ".join(Path(f).name for f in gap.source_files[:3])
            if len(gap.source_files) > 3:
                sources += f" (+{len(gap.source_files) - 3})"
            priority = self._calc_priority(gap.count)
            lines.append(f"| {gap.text} | {gap.count} | {priority} | {sources} |")

        return "\n".join(lines)

    def save(self, output_path: Path) -> None:
        """å„²å­˜å ±å‘Š"""
        output_path.write_text(self.generate_markdown(), encoding="utf-8")

    def save_json(self, output_path: Path) -> None:
        """å„²å­˜ JSON æ ¼å¼å ±å‘Š"""
        output_path.write_text(
            json.dumps(self.to_json(), ensure_ascii=False, indent=2),
            encoding="utf-8"
        )


class PendingLinkReplacer:
    """å¾…å»ºç«‹é€£çµæ›¿æ›å™¨"""

    def __init__(self, index: CardIndex, zettelkasten_dir: Path):
        self.index = index
        self.zettelkasten_dir = zettelkasten_dir
        self.gaps = GapReport()
        self.stats = {
            "files_scanned": 0,
            "files_modified": 0,
            "links_replaced": 0,
            "links_not_found": 0
        }

    def process_all(self, category_filter: str = None, dry_run: bool = True, quiet: bool = False) -> None:
        """è™•ç†æ‰€æœ‰å¡ç‰‡"""
        for category_dir in sorted(self.zettelkasten_dir.iterdir()):
            if not category_dir.is_dir() or category_dir.name in EXCLUDE_DIRS:
                continue

            if category_filter and category_dir.name != category_filter:
                continue

            for card_file in sorted(category_dir.glob("*.md")):
                if card_file.name in ("index.md", "_index.md"):
                    continue

                self._process_file(card_file, dry_run, quiet)

    def _process_file(self, file_path: Path, dry_run: bool, quiet: bool) -> None:
        """è™•ç†å–®ä¸€æª”æ¡ˆ"""
        self.stats["files_scanned"] += 1

        try:
            content = file_path.read_text(encoding="utf-8")
            lines = content.split("\n")
        except Exception as e:
            if not quiet:
                print(f"âŒ è®€å–å¤±æ•— {file_path}: {e}", file=sys.stderr)
            return

        modified = False
        new_lines = []

        for line_num, line in enumerate(lines, start=1):
            new_line = self._process_line(line, file_path, line_num, quiet)
            if new_line != line:
                modified = True
            new_lines.append(new_line)

        if modified:
            self.stats["files_modified"] += 1
            if not dry_run:
                file_path.write_text("\n".join(new_lines), encoding="utf-8")
            elif not quiet:
                print(f"ğŸ“ {file_path.relative_to(PROJECT_ROOT)}")

    def _process_line(self, line: str, file_path: Path, line_num: int, quiet: bool) -> str:
        """è™•ç†å–®ä¸€è¡Œï¼Œè¿”å›æ›¿æ›å¾Œçš„è¡Œ"""
        new_line = line

        # æ¨¡å¼ 1ï¼š[æ–‡å­—](å¾…å»ºç«‹)
        for match in reversed(list(PENDING_PATTERNS[0].finditer(line))):
            link_text = match.group(1)
            card = self.index.find_card(link_text)

            if card:
                rel_path = self._compute_relative_path(file_path, card.path)
                replacement = f"[{link_text}]({rel_path})"
                new_line = new_line[:match.start()] + replacement + new_line[match.end():]
                self.stats["links_replaced"] += 1
                if not quiet:
                    print(f"  âœ“ L{line_num}: [{link_text}] â†’ {rel_path}")
            else:
                self.gaps.add(link_text, str(file_path))
                self.stats["links_not_found"] += 1

        # æ¨¡å¼ 2ï¼š[å¾…å»ºç«‹: xxx](...) - é€šå¸¸ä¸æ›¿æ›ï¼Œåªè¨˜éŒ„
        for match in PENDING_PATTERNS[1].finditer(line):
            link_text = match.group(1)
            if link_text:
                card = self.index.find_card(link_text)
                if not card:
                    self.gaps.add(link_text, str(file_path))
                    self.stats["links_not_found"] += 1

        # æ¨¡å¼ 3ï¼š[æ–‡å­—](path)ï¼ˆå¾…å»ºç«‹ï¼‰- ç§»é™¤å°¾éƒ¨çš„ï¼ˆå¾…å»ºç«‹ï¼‰æ¨™è¨˜
        for match in reversed(list(PENDING_PATTERNS[2].finditer(line))):
            # æª¢æŸ¥é€£çµæ˜¯å¦æœ‰æ•ˆ
            link_part = match.group(1)
            # åªç§»é™¤ï¼ˆå¾…å»ºç«‹ï¼‰æ¨™è¨˜ï¼Œä¿ç•™é€£çµ
            new_line = new_line[:match.start()] + link_part + new_line[match.end():]

        return new_line

    def _compute_relative_path(self, source: Path, target: Path) -> str:
        """è¨ˆç®—ç›¸å°è·¯å¾‘"""
        source_cat = source.parent.name
        target_cat = target.parent.name

        if source_cat == target_cat:
            return target.name
        else:
            return f"../{target_cat}/{target.name}"


def print_stats(replacer: PendingLinkReplacer, quiet: bool = False) -> None:
    """è¼¸å‡ºçµ±è¨ˆè³‡è¨Š"""
    if quiet:
        return

    print("\n" + "=" * 60)
    print("ğŸ“Š çµ±è¨ˆçµæœ")
    print("=" * 60)
    print(f"  æƒææª”æ¡ˆæ•¸ï¼š{replacer.stats['files_scanned']}")
    print(f"  ä¿®æ”¹æª”æ¡ˆæ•¸ï¼š{replacer.stats['files_modified']}")
    print(f"  æ›¿æ›é€£çµæ•¸ï¼š{replacer.stats['links_replaced']}")
    print(f"  æœªæ‰¾åˆ°é€£çµï¼š{replacer.stats['links_not_found']}")
    print(f"  ç¼ºå£è©å½™æ•¸ï¼š{len(replacer.gaps.gaps)}")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="è‡ªå‹•æ›¿æ›ã€Œå¾…å»ºç«‹ã€é€£çµæ¨™è¨˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å·¥ä½œæµç¨‹æ•´åˆç¯„ä¾‹ï¼š
  # éšæ®µ 3ï¼šLink Building å®Œæ•´è™•ç†
  uv run scripts/replace_pending_links.py --fix --add-to-csv --report

  # åªç”Ÿæˆ JSON æ ¼å¼ç¼ºå£æ¸…å–®
  uv run scripts/replace_pending_links.py --fix --json > gaps.json
        """
    )

    parser.add_argument("--check", action="store_true",
                        help="æª¢æŸ¥æ¨¡å¼ï¼šçµ±è¨ˆå¾…å»ºç«‹é€£çµæ•¸é‡ï¼ˆé è¨­ï¼‰")
    parser.add_argument("--dry-run", action="store_true",
                        help="é è¦½è®Šæ›´ï¼Œä¸å¯¦éš›ä¿®æ”¹æª”æ¡ˆ")
    parser.add_argument("--fix", action="store_true",
                        help="åŸ·è¡Œæ›¿æ›")
    parser.add_argument("--report", action="store_true",
                        help="ç”Ÿæˆç¼ºå£å ±å‘Šåˆ° doc/worklog/ï¼ˆMarkdown æ ¼å¼ï¼‰")
    parser.add_argument("--json", action="store_true",
                        help="è¼¸å‡º JSON æ ¼å¼çš„ç¼ºå£æ¸…å–®")
    parser.add_argument("--add-to-csv", action="store_true",
                        help="ç›´æ¥æ–°å¢ç¼ºå£åˆ° Active CSV")
    parser.add_argument("--category", type=str,
                        help="åªè™•ç†ç‰¹å®šåˆ†é¡")
    parser.add_argument("--quiet", action="store_true",
                        help="éœé»˜æ¨¡å¼")

    args = parser.parse_args()

    # é è¨­ç‚º check æ¨¡å¼
    if not (args.dry_run or args.fix):
        args.check = True

    # 1. å»ºç«‹ç´¢å¼•
    if not args.quiet:
        print("ğŸ” å»ºç«‹å¡ç‰‡ç´¢å¼•...")

    index = CardIndex(ZETTELKASTEN_DIR)
    index.build_index()

    if not args.quiet:
        print(f"  ç´¢å¼•å®Œæˆï¼š{index.total_cards} å¼µå¡ç‰‡")
        print(f"  ä¸»ç´¢å¼•ï¼š{len(index.japanese_to_card)} é …")
        print(f"  åˆ¥åç´¢å¼•ï¼š{len(index.alias_to_card)} é …")

    # 2. è™•ç†å¡ç‰‡
    if not args.quiet:
        mode = "fix" if args.fix else "dry-run" if args.dry_run else "check"
        print(f"\nğŸ“ è™•ç†æ¨¡å¼ï¼š{mode}")

    replacer = PendingLinkReplacer(index, ZETTELKASTEN_DIR)
    dry_run = not args.fix
    replacer.process_all(args.category, dry_run, args.quiet)

    # 3. è¼¸å‡ºçµ±è¨ˆ
    if not args.json:  # JSON æ¨¡å¼ä¸‹ä¸è¼¸å‡ºçµ±è¨ˆ
        print_stats(replacer, args.quiet)

    # 4. JSON è¼¸å‡º
    if args.json:
        gaps_json = replacer.gaps.to_json()
        print(json.dumps(gaps_json, ensure_ascii=False, indent=2))
        sys.exit(0)

    # 5. ç”Ÿæˆ Markdown å ±å‘Š
    if args.report and len(replacer.gaps.gaps) > 0:
        report_dir = PROJECT_ROOT / "doc" / "worklog"
        report_dir.mkdir(parents=True, exist_ok=True)

        date_str = datetime.now().strftime("%Y%m%d")
        report_path = report_dir / f"gap-report-{date_str}.md"
        replacer.gaps.save(report_path)

        if not args.quiet:
            print(f"\nğŸ“„ ç¼ºå£å ±å‘Šå·²å„²å­˜ï¼š{report_path.relative_to(PROJECT_ROOT)}")

    # 6. æ–°å¢ç¼ºå£åˆ° CSV
    if args.add_to_csv and len(replacer.gaps.gaps) > 0:
        gaps_json = replacer.gaps.to_json()

        if len(gaps_json) > 0:
            # å¯«å…¥è‡¨æ™‚ JSON æª”æ¡ˆ
            import tempfile
            import subprocess

            with tempfile.NamedTemporaryFile(
                mode='w',
                suffix='.json',
                delete=False,
                encoding='utf-8'
            ) as f:
                json.dump(gaps_json, f, ensure_ascii=False, indent=2)
                temp_json_path = f.name

            try:
                # å‘¼å« add_pending_cards.py batchï¼ˆç›´æ¥ä½¿ç”¨ç•¶å‰ Python ç’°å¢ƒï¼‰
                # --csv-only æ˜¯ä¸»ç¨‹å¼é¸é …ï¼Œå¿…é ˆæ”¾åœ¨ batch å­å‘½ä»¤ä¹‹å‰
                result = subprocess.run(
                    [
                        sys.executable,
                        str(PROJECT_ROOT / "scripts" / "add_pending_cards.py"),
                        "--csv-only",  # åªæ–°å¢åˆ° CSVï¼Œä¸å»ºç«‹å¡ç‰‡æª”æ¡ˆ
                        "batch",
                        "--from-json", temp_json_path,
                    ],
                    capture_output=True,
                    text=True,
                    cwd=PROJECT_ROOT
                )

                if result.returncode == 0:
                    if not args.quiet:
                        print(f"\nâœ… å·²æ–°å¢ {len(gaps_json)} å€‹ç¼ºå£åˆ° Active CSV")
                else:
                    error_msg = result.stderr or result.stdout
                    print(f"\nâŒ æ–°å¢åˆ° CSV å¤±æ•—ï¼š{error_msg}", file=sys.stderr)
            finally:
                # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                Path(temp_json_path).unlink(missing_ok=True)
        else:
            if not args.quiet:
                print("\nâš ï¸ æ²’æœ‰æœ‰æ•ˆçš„ç¼ºå£éœ€è¦æ–°å¢")

    # è¿”å›ç‹€æ…‹
    sys.exit(0)


if __name__ == "__main__":
    main()
